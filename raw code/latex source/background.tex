\chapter{Background}
\label{chap:Background}
A few years ago I was avidly following algorithms in the news, what they were, but also how they were applied.  Were they being applied in an ethical manner?  The infamous trolley problem was recirculating because self-driving cars would need to make choices in the event of an impending crash.  Should a self-driving car protect its passengers life at the expense of pedestrians on the road?  Is there a balance of people and ages which tips this question in a particular direction?  Also freshly in the news, an algorithm used for criminal sentencing raised the question of if rescinding access to race data was enough to make a suitably flexible model race blind ~\citep{BOOK:5}.  Race was not explicitly hard-coded into the model, but could be reconstructed by a suitably flexible model which had access to enough features and a training set with judgements biased by human observation of race.  In teaching, should teachers being judged by algorithms be able to assess the validity of the algorithm they were being judged by.  In both cases, was there enough merit in not disclosing an algorithm's intellectual property to justify keeping the mechanisms hidden from the people whose lives were being affected.  In this manner, I came across an article by MathBabe about BISG ~\citep{WEBSITE:2}, Bayesian Improved Surname Geocoding, which led to a longer article on the Los Angeles Times~\citep{WEBSITE:1}.  Both articles describe BISG, an algorithm for guessing someone’s ethnicity from information that is legally obtainable about that individual.

\par BISG dates back to the late 2000s.  Marc Elliot, working for the Rand corporation, needed a better means of imputing missing race data for the sake of healthcare analysis.  Previous standards for estimating race used an individual’s place of residence or their last name, combined with the appropriate census statistics on residence or last name to estimate race.  In 2009, Elliot published findings that showed a Naive Bayes combination of location and surname information significantly improves the estimate of ethnicity by “41\% and 108\%” over “single-source surname and address methods”~\citep{WEBSITE:1}.  Because Elliot and his peers had access to an extensive healthcare network, they were able to demonstrate this with a health network database containing hundreds of thousands of self reported ethnicity statistics.

\par After BISG was published by the healthcare industry, it was picked up by the Consumer Finance Protection Bureau (CFPB) for doing analysis of the auto-lending industry.  Within the auto-lending industry, it is illegal to make loans based on someone’s ethnicity, and to that extent, it is illegal to ask for and obtain the ethnicity.  The CFPB wanted to know if the auto-lending industry was discriminating based on race, despite that this was prohibited, and then needed to review auto-loans to see if minority loans were indeed given less favorable loan terms.  Ironically, because race is not a valid assessment quality for loans, race data was not retained by the loan industry, and the CFPB had to devise a method to reverse engineer an estimate of ethnicity from other factors, last name and location of residence~\citep{ARTICLE:1}.  With this data, although highly politically contentious, the CFPB went on to use the BISG technique to show probable discrimination against minorities in the auto loan industry, and subsequently levied multi-million dollar fines based on that evidence.

\par Parallel to loans and healthcare, evictions and their effect on society have become a prevalent topic.  Matthew Desmond published “Evicted” in 2016, which argues that evictions are highly destructive, and disproportionately punishing to women with children~\citep{BOOK:6}.  Not only can evictions cause homelessness, but they can also induce poverty, creating a feedback loop which keeps people from lifting themselves up in society.  Exploring the implications of eviction being its own feedback loop, the Harvard Access to Justice Lab, a legal clinic partially involved in using technology to expand access to the legal system, subsequently asked if minorities are disparately impacted in the housing market?  First we need to unpack, what is disparate impact?  Disparity comes in at least two legal forms: disparate treatment, and disparate impact.  The former, disparate treatment, is taking an action which discriminates against a protected class; it is an active format of discrimination.  Disparate impact, however, is more subtle.  It is possible to take actions which, on the surface, do not portend to harm any section of the population, but still have unintended discriminatory effects.  Disparate impact is this unintentional cause of harm to a protected class through an otherwise unbiased rule or requirement~\citep{WEBSITE:5}.  This can subsequently be deemed illegal if there is not substantiated reason for the rules causing the discrimination.

\par An example of disparate impact could be given in the form of a new automotive factory in the 1960s which only hires applicants who have earned a PhD.  Because 77 percent of PhD graduates in the 1960s were white men, this policy would disproportionately favor employment of white men even though the policy does not intentionally target any specific ethnicity.  At this point, there is a question of if the imposed requirement is necessary to adequately fill the factory positions.  If the the requirement is imposed because the jobs for the factory demand engineering knowledge at the level of a PhD, then this could be a legal form of disparate impact.  If however, the job was for line workers who only needed to be physically fit and educated at a high school level, this would be unnecessarily selecting for a predominantly white male population of workers at the expense of other genders and ethnicities.  This latter case would be an example of illegal disparate impact since an unnecessary qualification would be causing the hiring bias.

\par While the example pertains to the disparate impact in the workplace, does disparate impact legally cover housing issues?  If yes, it would be an extension of the Fair Housing Act,  and this issue remained unsettled until this decade.  As recently as 2012, there were cases making their way through the court system which raised the issue of disparate impact applying in housing matters, but the cases were all settled out of court.  But, in 2015, a case did make it to the Supreme Court, Texas Department of Housing and Community Affairs v. Inclusive Communities Project, and by a 5-4 decision, the verdict was that the Fair Housing act did create cause for disparate impact to be applied to the housing sector ~\citep{WEBSITE:7}.

\par Bringing BISG and disparate impact together, attorneys local to Boston could not ascertain for certain what was happening in housing court, but observing the defendants in court, it was believed that defendants were minorities more often than they should have been.  The situation thus presented itself like in the auto-lending case.  Racial data is not kept in the official court records, so any analysis based on the race of evicted individuals has to be reconstructed from other information.  Despite that attorneys had a hunch, and despite that there was belief that local landlords used the public evictions records as justification for not renting to individuals, none of this had been proven.

\par Given the new standard for disparate impact, the immediate task is assessing if there is statistical proof of disparate impact.  If so, the first condition for determining illegal disparate impact would be satisfied, and potential actions could be taken to ameliorate any harm being done.