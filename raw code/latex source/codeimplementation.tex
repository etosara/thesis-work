\chapter{Code Implementation}
\label{chap:codeimplementation}
How to implement code became a big issue.  On the one hand, an underlying notion of Python is that you should not be spending your time implementing things that already exist in the language.  If you need to do a list sorting, you can implement that yourself, but you’re unlikely to have a better implementation unless you know a lot about the data you are working with, how it is structured, sorting in general, and how you might be able to find a better specific case algorithm to do the job.  Because of this, I tended to err in the direction of using off the shelf tools.  If I needed to lookup an address, I let the geocoder do what it was good at.  If I needed to search a table, I assumed that pandas had the best query functionality for finding things in its tables.  If I needed to resample a cohort of data, iterate through the rows multiple times.  While this was sufficient for original analysis, it presented problems.

\par The geocoder, when processing the entire data set, ran on a time scale of days.  At first I thought this was my old laptop.  I timed address look-ups, and at one second, I had to set up a batch of computations and let it run through the night and over the weekend.  As noted before, realizing this needed to be faster, I first tried running the computations on a faster machine hoping for an order of magnitude speedup.  Sadly, the faster machine didn’t translate into a faster lookup.  Because I didn’t need to clean the data often, this only went through a couple iterations.  While doing the look-ups sequentially, I realized that many addresses occurred more than once in the data set.  Not only were there frequently multiple tenants in a household, but often times addresses had evictions more than once.  Checking every search against a dictionary of prior searches and storing every search as it was made meant no search ever needed to be run more than once.  Not only did duplicate addresses not need to be searched, but faulty addresses didn’t need to be searched (note that false addresses usually take longer than correct addresses, presumably because there is more to search to verify that something doesn’t exist, especially given that there is some ambiguity on how a term can be searched).  This only resulted in a small speed up, about 30\%, but 30\% of multiple days is a noteworthy savings.

\par I did not realize this until later, but the real time savings on this would have been batch look-ups.  While the census geocoder allows for individual look-ups, it also has a feature that allows for batch look-ups of size up to ten thousand addresses at a time.  I tested the run time of this once I discovered it, and when combined with the time savings from eliminating the numerous repeat address, it is almost two orders of magnitude speed up, reducing multiple days of processing down to about two hours.  With parallel processing, this could easily be reduced to under an hour to achieve a full one-hundred fold improvement on run time.  This leads to the next topic.

\par Bootstrapping is the main technique I used in analyzing data.  At its heart, bootstrapping as a statistical tool requires one to resample their data repeatedly to get an empirical representation of the data.  When the technique was first invented in the late 1970s by Brad Efron, it wasn’t wholeheartedly embraced because computing power was limited, and analytical mathematical approaches were more wieldy if the required assumptions were met~\citep{WEBSITE:16}.  Now almost two decades into the next millennium, repeated sampling is a much easier task for a personal computer to undertake.

\par For most of the tests that I ran, I repeated a simulation one thousand times to get a good picture of the situation and adequately prepare for hypothesis testing.  Although this amount of repetition is considered a comparatively low amount of resampling when high accuracy is needed, almost none of the confidence intervals I constructed ended up needing high accuracy ~\citep{BOOK:3}.  Because the data frames were all roughly the same size, several early simulations shared a common single iteration run time around 1-2 seconds.  Running one thousand simulations meant the standard wait time for a test was in the ballpark of 15 to 20 minutes.

\begin{wrapfigure}{r}{2in}
\includegraphics[width=2in]{diagrams/processing.png}
\caption{Runtime Estimates as a Function of Cores}
\label{fig:figure1c}
\end{wrapfigure}

\par The obvious way to approach fixing this wait time is to use more computing power.  At its most rudimentary, I could queue up multiple Python notebooks and give them each a quarter of the work.  In a more sophisticated way, that’s what I did with Ipyparallel.  The software allows for multiple cores on a computer to be used for the same task, in parallel, while collecting the results in a central location for processing~\citep{WEBSITE:8}.  Instead of tasking one core on my machine with running one thousand simulations, I can task each of four cores with running two hundred fifty simulations.  Because the cores can run simultaneously, and each as is as fast as the other, the work takes roughly one quarter the time.  Hypothetically, if I queue up $n$ cores on my computer, I can reduce the work time by a factor of $n$.  In practice, the number of cores on a computer is limited, and in addition to running Python, the computer has others tasks to do.  If Python calculations take away from vital tasks elsewhere on the computer, it can be expected that periodically the computer will interrupt Python to do work that needs to be done.  Testing to see what that threshold was, I ran calculations on different numbers of cores to see where the point of steeply diminishing marginal returns was.  It ended up being between four and eight as the former saw a factor of four improvement, while the latter saw a factor of improvement only marginally better than that, and I opted to use four cores as my go-to cluster size for speed up.

\par The last and least glamorous major speed up comes from simple coding efficiency.  As noted before, I generally assume that code implemented in packages is fairly efficient, and is not something that I work too hard at out-performing.  Particularly with a library like Pandas which is widely used for data analysis, I assume that most operations are fast, and the bulk of time consumed is spent on the actual volume of work.  The catch here is that function calls take time, and the act of calling the function generally takes more time.  In the simulations I ran, I often needed to iterate over every row of a data frame, and perform a calculation for each row.  The naive approach to doing this one thousand times is to run through every row of the data frame, do one calculation in each row, and then repeat the whole process one thousand times.  This is very easy to think through and read through.  But, the numpy library was doing the bulk of the computations for me within each row iteration, and yielded a means of improving performance by reducing function calls.

\par Before optimization, I would iterate through M rows of a data frame, do one unit of work creating a 1x6 array per row.  At the end of every pass through the data frame, I would collapse the combined Mx6 dimensional array into a 1x6 dimensional array, and then repeat this process 1000 times until I effectively had a 1000x6 array, 1000 data points over 6 ethnicities.  As a radical improvement on this methodology, in any simulation that required every row of a data frame to be used, I could do 1000 units of work in each row with one function call and create a 1000x6 array.  After one pass through the data frame, I had an Mx1000x6 tensor which I could collapse into a 1000x6 array equivalent to that from the previous process.  This eliminated almost a thousand method calls to the data frame, and more than a million calls to the numpy random multinomial function.  Despite the added work with each call of the mutinomial function, this reduced run times from a hardware parallelized four minutes to an efficient code expedited ten seconds.
