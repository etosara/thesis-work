\chapter{Data Gathering}
\label{chap:datagathering}
The principal pieces of data gathering focused on the census records for demographics, the census records for surname ethnicity frequencies, and ultimately the eviction records.\par

The surname ethnicity records were the easiest to deal with.  Searching the Internet for census surname data will bring up hits on surname data from the last census.  Doing this again now, the data for the one hundred-sixty thousand most common last names from the 2010 census came up in a .csv file.  When I initially did this search, I had less luck and found the file as a Stata file.  I do not use Stata and didn't have a working knowledge of how to deal with this file type; this was at first an irritation, but quickly overcome.  Searching for how to open Stata files in python provided a stackexchange link and relevant code ~\cite{WEBSITE:13}.  Admittedly, now that I do not need this feature, and no longer use it, I also know that the Python Pandas library has a feature for reading Stata data, very simple and straightforward now when it was once more challenging. \par

The second part of data retrieval was eviction records.  The law lab that I was working with had extensive access to eviction records they had recorded for another project, but I didn’t have access to those records initially.  I decided I could scrape some data myself from the Massachusetts court website.  If you go to the website, it is an antiquated website, with nothing in the way of JavaScript loading aspects of the page after the fact.  It was a prime target for traditional data scraping using something like the beautifulsoup or requests library.  Simply create a program to accept the login, find the relevant boxes to be filled and searched, and then parse the results for links to data pages to be stored. \par
All of this was straight forward enough, but the MA court website has a strict policy of not allowing data scraping.  Their official stance is on the data served:

\begin{quote}
The case information contained within this web site is generated from computerized records maintained by the Massachusetts Trial Court and is deemed to be public information. While every effort is made to assure the data is accurate and current, it must be accepted and used by the recipient with the understanding that no warranties, expressed or implied, concerning the accuracy, reliability or suitability of this data have been made. The Trial Court, and the developers of this web site assume no liability whatsoever associated with the use or misuse of the data contained herein.  The case information from this web site is not the official record of the Trial Court. ~\citep{WEBSITE:12} 
\end{quote} \par

Access to the Massachusetts eAccess site by a site data harvester or any similar software intended to discover and extract data from a website through automated, repetitive querying for the purpose of collecting such data is expressly prohibited. \par

There are multiple relevant issues here.  At the end, there is the assertion that it is against the will of the court to write software to scrape content from the page.  Ethically, this ruled out two preferred scenarios.  One, write an extension for chrome which would either fully automate the search and downloading of data, or at least write an extension which would automatically open, save, and close a window once a list of links had been searched for and displayed on a page.  Two, writing a fully automated script to login and systematically download every case file by case number or type of case and date was also disallowed. \par

In the end, for the sake of partial automation and following the letter of the intended use, I would search for the desired criteria, open the pages 10 to 15 at a time, open them, and save them.  Running in the background, but not interacting with the site, I had a script running which would move the files to my desired local repository.  Once the files were saved to the local repository, they were opened up via beautifulsoup.  Sifting through the page’s DOM (hierarchical HTML structure), the immediately relevant information was extracted for further processing.  Originally this was the defendant name and address, but later included plaintiffs as well. \par

For this first round of data gathering, the need for data munging proved to be minimal to nonexistent.  The Eastern housing court, especially within the last two years, keeps very clean data records.  All addresses are complete.  Street number, name, and suffix are stored in separate fields, as were city and zip code.  Names were similarly consistently structured, “Last, First” with middle names or initials following the first name, but not being comma delimited. \par

All of the above consistencies made it very easy to scrape several hundred finalized eviction records from 2018.  I could download the data, and Python would transfer and turn the data into a dataframe and then a .csv file for me.  The hardest part of this, other than simply writing the code, was making sure I didn’t exceed the window opening rate the website had, or lingering on a page for too long.  Because there have been attempts to robotically scrape the website in the past, opening tabs was rate limited, and opening too many tabs/records too quickly would earn you a reminder that the website was not allowed to be robotically harvested.  As well, if I did a search with many results, it would eventually time out if I spent too long spot checking results on other pages, again with the reminder not to robotically scrape the page.  The final annoyance to circumvent here is that records searches would only provide the first 100 or so results.  Even if your search netted you 250 records (usually searching over a multi-day period), only the first piece of that search would be accessible.  Consequently, I was working with 25 records per page, following the pattern of load several results, save several results, and repeat until having iterated through the first 100.  Then, once arriving at the 100 record cutoff, run a new search from whenever the old records stopped displaying.  It was a bit slow, but still fast enough to net a few hundred records.

\par Ultimately, this set the stage for later work, but the several hundred data points I downloaded were obviated by the much larger data dump I received from the law school after my IRB training and after the “human subjects” issue was resolved.  The data set grew by more than two orders of magnitude, and I eventually had nearly two hundred thousand data points to start the assessment with.  One minor change was required with the new data set.  Managing the data was still html scraping at the heart of it, but details changed with switching from html only to web page complete files.  A quick change to the DOM parsing and beautifulsoup code rapidly turned my new 800MB dump of files into a usable table of data.

\par The third piece of data gathering, and very related to the records processing, was geocoding.  In one sense, turning someone’s census tract into an estimation of their ethnicity was straightforward.  Look at which census tract they live in.  Retrieve the ethnic breakdown for that neighborhood.  Assign those probabilities to a person.  The difficulty with this comes from not knowing what census tract someone lives in.  Using maps to find census tracts is completely infeasible for more than one or two people.  It turns out you can go to census.gov and they have a tool for identifying census tracts, but this is also too unwieldy and slow.  The preferred solution to this problem is a Python implementation of the census geocoder~\citep{WEBSITE:14}.  Taking the addresses parsed from the eviction records, those strings can be stitched back together and identified by the censusgeodcode library.

\par The censusgeocode library proved to be an annoying bottleneck in processing.  I started work on an a slow/archaic laptop.  I thought the laptop was old and that was what was rate limiting, so I tried a significantly newer and faster desktop.  To an extent, the old hardware had been part of the problem.  Unfortunately the processing rates continued to be slow on the new device.  This wasn’t a very big issue when I was dealing with the small amount of data I had downloaded by hand.  Geocoding took a couple hours.  But when I had one hundred-sixty thousand data points, that ran considerably slower.  Worse yet, if an address was a valid address, it took on average more than a second to find the census tract, but if an address couldn’t be found, it took a few more additional seconds for the software to determine that there was no good return value.  This meant that the first run through geocoding literally took days, and I lost about half of the data set to unfindable addresses.

\par After this first run through the data, and doing some cursory analysis, I decided it was time to do a better job cleaning the data and geocoding.  This data cleaning ended up being one of the more tedious aspects of the project.  The Mass Court system has admitted that they don’t want the court database to be scraped and analyzed for the sake of errors and anomalies in the data, and these errors are definitely present.  Of the first thousand or so eviction records, there are only 10 or so which can be salvaged.  Of the rest, often there is a number and a street listed, or a number and a city, but frequently not the street number, name, and suffix, city and zip code.  The city can be done without if the zip code exists, but without either, the data can not be geocoded.  Likewise, without the street number, name, or suffix, the data can not be accurately geocoded and must be ignored/removed.

\par Additional to omissions in address data are actual pieces of information which the geocoder does not know how to handle.  Addresses with letters attached to the numbers will not process.  Sequences of numbers (as sometimes eviction processings were for adjacent homes) would crash the system with the connecting hyphens.  Basement or lower level denotations don't register as numbers.  Municipalities within larger cities may not register.  Simple fixes exist for most of these problems.  Splitting the string which is the address will always give some form of a numeral in the first position.  If there are multiple addresses concatenated together, it was always by hyphen or slash.  Splitting an object containing a hyphen or slash and taking the preceding piece will give something that can be turned into a valid street number.  Checking for just the numeric part, and tossing out any letter associated with an apartment or condo will give a number that the census geocoder can handle.  While this may introduce a slight variation on the truth of where an eviction took place, it won’t change the census tract.

\par The hardest part of an address to clean up is the street name given its high variability.  Because there is an easy heuristic to process the street number, we started there.  Then working from the most general piece of the address, the zip code, state, and city are the next easiest things to process.  If the zip code exists, we keep it.  If the zip code does not exist we look to a city designation.  Checking the city is a bit trickier as we’re looking for something other than a string of 5 numerals.  This is still a somewhat straightforward problem to create a heuristic for.  Searching the internet for valid city names in Massachusetts, Wikipedia had a convenient list of cities~\citep{website:17}.  Without even needing to save the entire page, I could cut and paste the table, with associated values, and then edit out everything that wasn’t a city with some light cleaning.  Checking the list of cities in the data against the list of municipalities from Wikipedia, I could quickly ascertain that almost every city listed was a correctly entered functional city.  The exceptions included one eviction which appeared to regard a city in Connecticut and a few evictions in locations which were subordinate to larger municipalities (e.g. - East Arlington as opposed to Arlington).  In the case of the city in Connecticut, that data was removed from the data set.  In the case of subordinate municipalities, the East, West, or otherwise unneeded designation was removed.  Other cases were found where data points had no city or zip code, and the street had been improperly marked as the city; with no way to tell which were in the state a particular street was, all of these data points had to be discarded.

\par After a quick processing of the state designation (all data in the set were listed as MA), the last thing to process was the street addresses.  This ended up being a very manual process.  Picking apart a street name came with a multitude of variations.  Checking the two or three pieces of address I had left, and knowing this had to be street name or suffix, I started with the trailing piece, the suffix.

\par With high probability, for data points where I had more than two pieces of information to deal with, the last piece of information proved to be extraneous.  Often times the person logging the court records had made a designation like “basement” or “4a” or “side alley”.  All of these things would be useful for finding a residence in the real world, but not useful for geocoding.  I constructed a set of all the possible trailing pieces of information, and eliminated anything that was clearly not a street suffix.  There were thousands of strings which needed to be evaluated as street suffixes, but when ordered alphabetically, it was very easy to quickly sort through the list picking out the tens of options which were clearly suffixes, or plausible false positives for street suffixes.  Since the extraneous bits weren’t useful, the non-suffixes were removed while still being able to keep the remaining aspects of that data point. \par

Having cleaned all the “easy” parts of the addresses, the last part of data cleaning addresses was dealing with the principle street names.  Creating a set of these, and scrolling through them, the only obvious issue that remained was parenthetical notes which had been transcribed into the streets.  Checking these manually, the notes from the remaining data indicated duplicate addresses.  When sometimes a building lived at the intersection of two streets, two addresses could be given.  Because these locations were literally describing the same buildings, parenthetical notes were removed.

\par At the point where an attempt to clean all aspects of an address had been made, what remained was fed through the census geocoder.  While not all constructs would be deemed valid addresses, it wasn’t (at least initially) worth the effort to refine the data cleaning any further.  If it took forty-four hours to have the census geocoder run through the data, any reruns through that data would waste otherwise valuable time.  Alternatively, spending a minute or more to decipher and manually look up an address becomes an egregious use of time when you have more than one hundred thousand valid data points, and even if it’s a mere one thousand data points that need looking into, at a one per minute rate, that’s multiple work days spent solely looking up addresses, many of which will not be resolvable.